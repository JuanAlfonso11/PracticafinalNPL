{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80811e2d",
   "metadata": {},
   "source": [
    "<div style=\"background:#FFFFE0;padding:20px;color:#000000;margin-top:10px;\">\n",
    "Imports necesarios para la ejecución de los módulos instalados con pip:\n",
    "\n",
    "• pandas → import pandas as pd  \n",
    "• numpy → import numpy as np  \n",
    "• matplotlib → import matplotlib.pyplot as plt  \n",
    "• seaborn → import seaborn as sns  \n",
    "• scikit-learn → from sklearn.model_selection import train_test_split  \n",
    "                    from sklearn.metrics import classification_report, confusion_matrix  \n",
    "• torch → import torch  \n",
    "• transformers → from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding  \n",
    "• datasets → from datasets import Dataset  \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b657d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install transformers datasets scikit-learn pandas matplotlib seaborn torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692cb8fa",
   "metadata": {},
   "source": [
    "<div style=\"background:#FFFFE0;padding:20px;color:#000000;margin-top:10px;\">\n",
    "Este bloque de código verifica si PyTorch puede usar la GPU (usualmente con CUDA) y cuál GPU está disponible. Es útil para asegurarse de que el entrenamiento del modelo se pueda hacer con aceleración por hardware, lo que reduce significativamente el tiempo.</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b851e5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\"CUDA disponible:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdec902a",
   "metadata": {},
   "source": [
    "# Clasificacion automatica de poemas segun su forma poetica(usando la carpeta forms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41fd639c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\scout\\anaconda3\\envs\\nlp-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from transformers import DataCollatorWithPadding\n",
    "from datasets import Dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "373ddf2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text label\n",
      "0  2 ABC of H.k. and China revised vision.\\nBarre...   abc\n",
      "1  Apparently life without love, is no life at al...   abc\n",
      "2  A abc angles on angels flaws (poem)\\nMix with ...   abc\n",
      "3  A abc Brazil dance (poem)\\nJack of crack in po...   abc\n",
      "4  ABC... I can't go on\\n123... what's the next o...   abc\n",
      "label\n",
      "acrostic                       100\n",
      "allegory                       100\n",
      "free-verse                     100\n",
      "cinquain                       100\n",
      "cavatina                       100\n",
      "ballad                         100\n",
      "ballade                        100\n",
      "tetractys                      100\n",
      "triolet                        100\n",
      "villanelle                     100\n",
      "stanza                         100\n",
      "syllabic-verse                 100\n",
      "epigram                        100\n",
      "dirge                          100\n",
      "clerihew                       100\n",
      "epitaph                        100\n",
      "elegy                          100\n",
      "epistle                        100\n",
      "verse                          100\n",
      "monoku                         100\n",
      "lament                         100\n",
      "italian-sonnet                 100\n",
      "hymn                           100\n",
      "tanka                          100\n",
      "couplet                         99\n",
      "pastoral                        99\n",
      "doggerel                        99\n",
      "abc                             99\n",
      "sestina                         99\n",
      "riddle                          99\n",
      "lyric                           99\n",
      "carol                           99\n",
      "epic                            99\n",
      "ghazal                          99\n",
      "haiku                           99\n",
      "limerick                        98\n",
      "quatrain                        98\n",
      "ode                             97\n",
      "imagery                         96\n",
      "pantoum                         96\n",
      "rondeau                         90\n",
      "dactyl                          83\n",
      "sonnet                          79\n",
      "aubade                          77\n",
      "prose-poem                      77\n",
      "bio                             76\n",
      "narrative                       70\n",
      "madrigal                        68\n",
      "tercet                          66\n",
      "octave                          64\n",
      "rictameter                      63\n",
      "found-poem                      62\n",
      "slam                            60\n",
      "tyburn                          59\n",
      "ars-poetica                     57\n",
      "heroic-couplet                  55\n",
      "kyrielle                        54\n",
      "chain-verse                     52\n",
      "eclogue                         51\n",
      "sijo                            49\n",
      "conceit                         49\n",
      "cacophony                       48\n",
      "cascade                         44\n",
      "quatern                         36\n",
      "anaphora                        30\n",
      "lay                             29\n",
      "anacreontic                     29\n",
      "blank-verse                     29\n",
      "sestet                          28\n",
      "anagram                         26\n",
      "nonet                           26\n",
      "shi                             25\n",
      "light-verse                     25\n",
      "renga                           25\n",
      "bop                             24\n",
      "shadorma                        24\n",
      "ekphrastic                      22\n",
      "terza-rima                      22\n",
      "abecedarian                     20\n",
      "iambic-pentameter               20\n",
      "burlesque                       19\n",
      "canzone                         19\n",
      "choka                           18\n",
      "rhyme-royal-or-rime-royale      18\n",
      "catena-rondo                    18\n",
      "blues-poem                      17\n",
      "bucolic                         17\n",
      "dizain                          17\n",
      "rispetto                        15\n",
      "dramatic-monologue              15\n",
      "ottava-rima                     15\n",
      "balassi-stanza                  14\n",
      "sapphic                         13\n",
      "palinode                        12\n",
      "cento                           12\n",
      "chance-operations               12\n",
      "alexandrine                     11\n",
      "double-dactyl                   11\n",
      "epithalamion                    10\n",
      "senryu                          10\n",
      "panegyric                       10\n",
      "verse-paragraph                  9\n",
      "kennings                         9\n",
      "somonka                          9\n",
      "palindrome-or-mirror-poetry      9\n",
      "refrain                          9\n",
      "landays                          8\n",
      "horatian-ode                     7\n",
      "arabian-sonnet                   6\n",
      "bref-double                      6\n",
      "curtal-sonnet                    6\n",
      "glosa                            5\n",
      "burns-stanza                     5\n",
      "spenserian-stanza                5\n",
      "canzonetta                       4\n",
      "decastich                        4\n",
      "echo-verse                       4\n",
      "beymorlin-sonnet                 4\n",
      "brisbane-sonnet                  4\n",
      "oulipo                           4\n",
      "qasida                           4\n",
      "rondel-or-roundel                4\n",
      "didactic-poetry                  3\n",
      "collins-sestet                   3\n",
      "carpe-diems                      3\n",
      "blues-sonnet                     2\n",
      "busta-sonetto                    2\n",
      "fourteener                       2\n",
      "epistrophe                       2\n",
      "divino-sonetto                   2\n",
      "occasional-poem                  2\n",
      "pindaric-ode                     2\n",
      "mock-epic                        1\n",
      "irregular-ode                    1\n",
      "triversen                        1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "ruta_base = \"archive/forms\"\n",
    "\n",
    "# Inicializamos listas vacías\n",
    "textos = []\n",
    "etiquetas = []\n",
    "\n",
    "# Recorrer cada carpeta (que es una clase)\n",
    "for nombre_carpeta in os.listdir(ruta_base):\n",
    "    ruta_carpeta = os.path.join(ruta_base, nombre_carpeta)\n",
    "    if os.path.isdir(ruta_carpeta):\n",
    "        for archivo in os.listdir(ruta_carpeta):\n",
    "            ruta_archivo = os.path.join(ruta_carpeta, archivo)\n",
    "            try:\n",
    "                with open(ruta_archivo, 'r', encoding='utf-8') as f:\n",
    "                    contenido = f.read().strip()\n",
    "                    textos.append(contenido)\n",
    "                    etiquetas.append(nombre_carpeta)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "# Crear el DataFrame\n",
    "df = pd.DataFrame({'text': textos, 'label': etiquetas})\n",
    "\n",
    "# Ver los primeros datos\n",
    "print(df.head())\n",
    "print(df['label'].value_counts().to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7f7b282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label  label_id\n",
      "0  haiku one:\\nin criss-crossing shadows\\nof the ...  haiku         0\n",
      "1  war memorial\\nreccuring, late dad's words\\ntha...  haiku         0\n",
      "2  I’m down: <(here’s the news\\nJust cause I’m wh...  haiku         0\n",
      "3  The harvest soon comes\\nA hawk's eye is needed...  haiku         0\n",
      "4  I'm here\\namongst the huge pile of haiku.\\nCan...  haiku         0\n",
      "label\n",
      "haiku     99\n",
      "sonnet    79\n",
      "Name: count, dtype: int64\n",
      "label_id\n",
      "0    99\n",
      "1    79\n",
      "Name: count, dtype: int64\n",
      "['haiku' 'sonnet']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "clases_deseadas = ['haiku', 'sonnet']\n",
    "df_binario = df[df['label'].isin(clases_deseadas)].reset_index(drop=True)\n",
    "\n",
    "le = LabelEncoder()\n",
    "df_binario['label_id'] = le.fit_transform(df_binario['label'])\n",
    "\n",
    "print(df_binario.head())\n",
    "print(df_binario['label'].value_counts())\n",
    "print(df_binario['label_id'].value_counts())\n",
    "print(le.classes_)  # para ver cuál es 0 y cuál es 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d8581b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 178/178 [00:00<00:00, 664.98 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Cargar el tokenizer de BERT\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=128)\n",
    "\n",
    "dataset = Dataset.from_pandas(df_binario[['text', 'label_id']])\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60311ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenamiento: 142 ejemplos\n",
      "Evaluación: 36 ejemplos\n"
     ]
    }
   ],
   "source": [
    "# Dividir en entrenamiento y prueba (80% - 20%)\n",
    "split_dataset = tokenized_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# Asignar a variables por claridad\n",
    "train_dataset = split_dataset['train']\n",
    "eval_dataset = split_dataset['test']\n",
    "\n",
    "# Confirmar tamaños\n",
    "print(f\"Entrenamiento: {len(train_dataset)} ejemplos\")\n",
    "print(f\"Evaluación: {len(eval_dataset)} ejemplos\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bbe3ca2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "# Cargar modelo BERT para clasificación (2 clases)\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
