{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80811e2d",
   "metadata": {},
   "source": [
    "<div style=\"background:#FFFFE0;padding:20px;color:#000000;margin-top:10px;\">\n",
    "Imports necesarios para la ejecución de los módulos instalados con pip:\n",
    "\n",
    "• pandas → import pandas as pd  \n",
    "• numpy → import numpy as np  \n",
    "• matplotlib → import matplotlib.pyplot as plt  \n",
    "• seaborn → import seaborn as sns  \n",
    "• scikit-learn → from sklearn.model_selection import train_test_split  \n",
    "                    from sklearn.metrics import classification_report, confusion_matrix  \n",
    "• torch → import torch  \n",
    "• transformers → from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding  \n",
    "• datasets → from datasets import Dataset  \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b657d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install transformers datasets scikit-learn pandas matplotlib seaborn torch\n",
    "#!pip install \"transformers[torch]\" --upgrade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee116fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install \"transformers[torch]\" --upgrade\n",
    "#!pip install --upgrade transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a3e1816",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade transformers accelerate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692cb8fa",
   "metadata": {},
   "source": [
    "<div style=\"background:#FFFFE0;padding:20px;color:#000000;margin-top:10px;\">\n",
    "Este bloque de código verifica si PyTorch puede usar la GPU (usualmente con CUDA) y cuál GPU está disponible. Es útil para asegurarse de que el entrenamiento del modelo se pueda hacer con aceleración por hardware, lo que reduce significativamente el tiempo.</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b851e5bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA disponible: True\n",
      "GPU: NVIDIA GeForce RTX 3050 Ti Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"CUDA disponible:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdec902a",
   "metadata": {},
   "source": [
    "# Clasificacion automatica de poemas segun su forma poetica(usando la carpeta forms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41fd639c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\scout\\anaconda3\\envs\\nlp-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from transformers import DataCollatorWithPadding\n",
    "from datasets import Dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "373ddf2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text label\n",
      "0  2 ABC of H.k. and China revised vision.\\nBarre...   abc\n",
      "1  Apparently life without love, is no life at al...   abc\n",
      "2  A abc angles on angels flaws (poem)\\nMix with ...   abc\n",
      "3  A abc Brazil dance (poem)\\nJack of crack in po...   abc\n",
      "4  ABC... I can't go on\\n123... what's the next o...   abc\n",
      "label\n",
      "acrostic                       100\n",
      "allegory                       100\n",
      "free-verse                     100\n",
      "cinquain                       100\n",
      "cavatina                       100\n",
      "ballad                         100\n",
      "ballade                        100\n",
      "tetractys                      100\n",
      "triolet                        100\n",
      "villanelle                     100\n",
      "stanza                         100\n",
      "syllabic-verse                 100\n",
      "epigram                        100\n",
      "dirge                          100\n",
      "clerihew                       100\n",
      "epitaph                        100\n",
      "elegy                          100\n",
      "epistle                        100\n",
      "verse                          100\n",
      "monoku                         100\n",
      "lament                         100\n",
      "italian-sonnet                 100\n",
      "hymn                           100\n",
      "tanka                          100\n",
      "couplet                         99\n",
      "sestina                         99\n",
      "doggerel                        99\n",
      "abc                             99\n",
      "epic                            99\n",
      "riddle                          99\n",
      "lyric                           99\n",
      "carol                           99\n",
      "haiku                           99\n",
      "ghazal                          99\n",
      "pastoral                        98\n",
      "limerick                        98\n",
      "quatrain                        98\n",
      "ode                             97\n",
      "imagery                         96\n",
      "pantoum                         96\n",
      "rondeau                         90\n",
      "dactyl                          83\n",
      "sonnet                          79\n",
      "aubade                          77\n",
      "prose-poem                      77\n",
      "bio                             76\n",
      "narrative                       70\n",
      "madrigal                        68\n",
      "tercet                          66\n",
      "octave                          64\n",
      "rictameter                      63\n",
      "found-poem                      61\n",
      "slam                            59\n",
      "tyburn                          59\n",
      "ars-poetica                     57\n",
      "heroic-couplet                  55\n",
      "kyrielle                        54\n",
      "chain-verse                     52\n",
      "eclogue                         51\n",
      "sijo                            49\n",
      "conceit                         49\n",
      "cacophony                       48\n",
      "cascade                         44\n",
      "quatern                         36\n",
      "anaphora                        30\n",
      "lay                             29\n",
      "anacreontic                     29\n",
      "blank-verse                     29\n",
      "sestet                          28\n",
      "anagram                         26\n",
      "nonet                           26\n",
      "shi                             25\n",
      "light-verse                     25\n",
      "renga                           25\n",
      "bop                             24\n",
      "shadorma                        24\n",
      "ekphrastic                      22\n",
      "terza-rima                      22\n",
      "abecedarian                     20\n",
      "iambic-pentameter               20\n",
      "burlesque                       19\n",
      "canzone                         19\n",
      "choka                           18\n",
      "rhyme-royal-or-rime-royale      18\n",
      "catena-rondo                    18\n",
      "blues-poem                      17\n",
      "bucolic                         17\n",
      "dizain                          17\n",
      "rispetto                        15\n",
      "dramatic-monologue              15\n",
      "ottava-rima                     15\n",
      "balassi-stanza                  14\n",
      "sapphic                         13\n",
      "palinode                        12\n",
      "cento                           12\n",
      "chance-operations               12\n",
      "alexandrine                     11\n",
      "double-dactyl                   11\n",
      "epithalamion                    10\n",
      "senryu                          10\n",
      "panegyric                       10\n",
      "verse-paragraph                  9\n",
      "kennings                         9\n",
      "somonka                          9\n",
      "palindrome-or-mirror-poetry      9\n",
      "refrain                          9\n",
      "landays                          8\n",
      "horatian-ode                     7\n",
      "arabian-sonnet                   6\n",
      "bref-double                      6\n",
      "curtal-sonnet                    6\n",
      "glosa                            5\n",
      "burns-stanza                     5\n",
      "spenserian-stanza                5\n",
      "canzonetta                       4\n",
      "decastich                        4\n",
      "echo-verse                       4\n",
      "beymorlin-sonnet                 4\n",
      "brisbane-sonnet                  4\n",
      "oulipo                           4\n",
      "qasida                           4\n",
      "rondel-or-roundel                4\n",
      "didactic-poetry                  3\n",
      "collins-sestet                   3\n",
      "carpe-diems                      3\n",
      "blues-sonnet                     2\n",
      "busta-sonetto                    2\n",
      "fourteener                       2\n",
      "epistrophe                       2\n",
      "divino-sonetto                   2\n",
      "occasional-poem                  2\n",
      "pindaric-ode                     2\n",
      "mock-epic                        1\n",
      "irregular-ode                    1\n",
      "triversen                        1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "ruta_base = \"archive/forms\"\n",
    "\n",
    "# Inicializamos listas vacías\n",
    "textos = []\n",
    "etiquetas = []\n",
    "\n",
    "# Recorrer cada carpeta (que es una clase)\n",
    "for nombre_carpeta in os.listdir(ruta_base):\n",
    "    ruta_carpeta = os.path.join(ruta_base, nombre_carpeta)\n",
    "    if os.path.isdir(ruta_carpeta):\n",
    "        for archivo in os.listdir(ruta_carpeta):\n",
    "            ruta_archivo = os.path.join(ruta_carpeta, archivo)\n",
    "            try:\n",
    "                with open(ruta_archivo, 'r', encoding='utf-8') as f:\n",
    "                    contenido = f.read().strip()\n",
    "                    textos.append(contenido)\n",
    "                    etiquetas.append(nombre_carpeta)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "# Crear el DataFrame\n",
    "df = pd.DataFrame({'text': textos, 'label': etiquetas})\n",
    "\n",
    "# Ver los primeros datos\n",
    "print(df.head())\n",
    "print(df['label'].value_counts().to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7f7b282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label  label_id\n",
      "0  haiku one:\\nin criss-crossing shadows\\nof the ...  haiku         0\n",
      "1  war memorial\\nreccuring, late dad's words\\ntha...  haiku         0\n",
      "2  I’m down: <(here’s the news\\nJust cause I’m wh...  haiku         0\n",
      "3  The harvest soon comes\\nA hawk's eye is needed...  haiku         0\n",
      "4  I'm here\\namongst the huge pile of haiku.\\nCan...  haiku         0\n",
      "label\n",
      "haiku     99\n",
      "sonnet    79\n",
      "Name: count, dtype: int64\n",
      "label_id\n",
      "0    99\n",
      "1    79\n",
      "Name: count, dtype: int64\n",
      "['haiku' 'sonnet']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "clases_deseadas = ['haiku', 'sonnet']\n",
    "df_binario = df[df['label'].isin(clases_deseadas)].reset_index(drop=True)\n",
    "\n",
    "le = LabelEncoder()\n",
    "df_binario['label_id'] = le.fit_transform(df_binario['label'])\n",
    "\n",
    "print(df_binario.head())\n",
    "print(df_binario['label'].value_counts())\n",
    "print(df_binario['label_id'].value_counts())\n",
    "print(le.classes_)  # para ver cuál es 0 y cuál es 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d8581b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 178/178 [00:00<00:00, 646.15 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Cargar el tokenizer de BERT\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=128)\n",
    "\n",
    "dataset = Dataset.from_pandas(df_binario[['text', 'label_id']])\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.rename_column(\"label_id\", \"labels\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60311ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenamiento: 142 ejemplos\n",
      "Evaluación: 36 ejemplos\n"
     ]
    }
   ],
   "source": [
    "# Dividir en entrenamiento y prueba (80% - 20%)\n",
    "split_dataset = tokenized_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# Asignar a variables por claridad\n",
    "train_dataset = split_dataset['train']\n",
    "eval_dataset = split_dataset['test']\n",
    "\n",
    "# Confirmar tamaños\n",
    "print(f\"Entrenamiento: {len(train_dataset)} ejemplos\")\n",
    "print(f\"Evaluación: {len(eval_dataset)} ejemplos\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbe3ca2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "# Cargar modelo BERT para clasificación (2 clases)\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f92b199",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scout\\AppData\\Local\\Temp\\ipykernel_29060\\950241779.py:17: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "c:\\Users\\scout\\anaconda3\\envs\\nlp-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='45' max='54' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [45/54 00:08 < 00:01, 4.88 it/s, Epoch 2.44/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     17\u001b[39m trainer = Trainer(\n\u001b[32m     18\u001b[39m     model=model,\n\u001b[32m     19\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m     23\u001b[39m     data_collator=data_collator,\n\u001b[32m     24\u001b[39m )\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Entrenar\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\scout\\anaconda3\\envs\\nlp-env\\Lib\\site-packages\\transformers\\trainer.py:2237\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2235\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2236\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2237\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2238\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2239\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2240\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2241\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2242\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\scout\\anaconda3\\envs\\nlp-env\\Lib\\site-packages\\transformers\\trainer.py:2532\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2530\u001b[39m update_step += \u001b[32m1\u001b[39m\n\u001b[32m   2531\u001b[39m num_batches = args.gradient_accumulation_steps \u001b[38;5;28;01mif\u001b[39;00m update_step != (total_updates - \u001b[32m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m remainder\n\u001b[32m-> \u001b[39m\u001b[32m2532\u001b[39m batch_samples, num_items_in_batch = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_batch_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2533\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch_samples):\n\u001b[32m   2534\u001b[39m     step += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\scout\\anaconda3\\envs\\nlp-env\\Lib\\site-packages\\transformers\\trainer.py:5343\u001b[39m, in \u001b[36mTrainer.get_batch_samples\u001b[39m\u001b[34m(self, epoch_iterator, num_batches, device)\u001b[39m\n\u001b[32m   5341\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[32m   5342\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m5343\u001b[39m         batch_samples.append(\u001b[38;5;28mnext\u001b[39m(epoch_iterator))\n\u001b[32m   5344\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m   5345\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\scout\\anaconda3\\envs\\nlp-env\\Lib\\site-packages\\accelerate\\data_loader.py:577\u001b[39m, in \u001b[36mDataLoaderShard.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    574\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    575\u001b[39m     \u001b[38;5;66;03m# But we still move it to the device so it is done before `StopIteration` is reached\u001b[39;00m\n\u001b[32m    576\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m577\u001b[39m         current_batch = \u001b[43msend_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_non_blocking\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    578\u001b[39m     \u001b[38;5;28mself\u001b[39m._update_state_dict()\n\u001b[32m    579\u001b[39m     next_batch = \u001b[38;5;28mnext\u001b[39m(dataloader_iter)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\scout\\anaconda3\\envs\\nlp-env\\Lib\\site-packages\\accelerate\\utils\\operations.py:153\u001b[39m, in \u001b[36msend_to_device\u001b[39m\u001b[34m(tensor, device, non_blocking, skip_keys)\u001b[39m\n\u001b[32m    151\u001b[39m     device = \u001b[33m\"\u001b[39m\u001b[33mnpu:0\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:  \u001b[38;5;66;03m# .to() doesn't accept non_blocking as kwarg\u001b[39;00m\n\u001b[32m    155\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor.to(device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\scout\\anaconda3\\envs\\nlp-env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:809\u001b[39m, in \u001b[36mBatchEncoding.to\u001b[39m\u001b[34m(self, device, non_blocking)\u001b[39m\n\u001b[32m    805\u001b[39m \u001b[38;5;66;03m# This check catches things like APEX blindly calling \"to\" on all inputs to a module\u001b[39;00m\n\u001b[32m    806\u001b[39m \u001b[38;5;66;03m# Otherwise it passes the casts down and casts the LongTensor containing the token idxs\u001b[39;00m\n\u001b[32m    807\u001b[39m \u001b[38;5;66;03m# into a HalfTensor\u001b[39;00m\n\u001b[32m    808\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m is_torch_device(device) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mint\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m809\u001b[39m     \u001b[38;5;28mself\u001b[39m.data = \u001b[43m{\u001b[49m\n\u001b[32m    810\u001b[39m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mhasattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mcallable\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\n\u001b[32m    811\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    812\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m    813\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    814\u001b[39m     logger.warning(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAttempting to cast a BatchEncoding to type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(device)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. This is not supported.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\scout\\anaconda3\\envs\\nlp-env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:810\u001b[39m, in \u001b[36m<dictcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    805\u001b[39m \u001b[38;5;66;03m# This check catches things like APEX blindly calling \"to\" on all inputs to a module\u001b[39;00m\n\u001b[32m    806\u001b[39m \u001b[38;5;66;03m# Otherwise it passes the casts down and casts the LongTensor containing the token idxs\u001b[39;00m\n\u001b[32m    807\u001b[39m \u001b[38;5;66;03m# into a HalfTensor\u001b[39;00m\n\u001b[32m    808\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m is_torch_device(device) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mint\u001b[39m):\n\u001b[32m    809\u001b[39m     \u001b[38;5;28mself\u001b[39m.data = {\n\u001b[32m--> \u001b[39m\u001b[32m810\u001b[39m         k: \u001b[43mv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(v, \u001b[33m\"\u001b[39m\u001b[33mto\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(v.to) \u001b[38;5;28;01melse\u001b[39;00m v\n\u001b[32m    811\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.data.items()\n\u001b[32m    812\u001b[39m     }\n\u001b[32m    813\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    814\u001b[39m     logger.warning(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAttempting to cast a BatchEncoding to type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(device)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. This is not supported.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "# Preparar colador de datos\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Argumentos básicos de entrenamiento compatibles\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./resultados\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs\"\n",
    ")\n",
    "\n",
    "# Definir Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Entrenar\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b54ebcc",
   "metadata": {},
   "outputs": [
    {
     "ename": "SafetensorError",
     "evalue": "Error while serializing: IoError(Os { code: 1224, kind: Uncategorized, message: \"The requested operation cannot be performed on a file with a user-mapped section open.\" })",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mSafetensorError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./modelo_poemas\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m tokenizer.save_pretrained(\u001b[33m\"\u001b[39m\u001b[33m./modelo_poemas\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\scout\\anaconda3\\envs\\nlp-env\\Lib\\site-packages\\transformers\\modeling_utils.py:4109\u001b[39m, in \u001b[36mPreTrainedModel.save_pretrained\u001b[39m\u001b[34m(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, safe_serialization, variant, token, save_peft_format, **kwargs)\u001b[39m\n\u001b[32m   4104\u001b[39m     gc.collect()\n\u001b[32m   4106\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m safe_serialization:\n\u001b[32m   4107\u001b[39m     \u001b[38;5;66;03m# At some point we will need to deal better with save_function (used for TPU and other distributed\u001b[39;00m\n\u001b[32m   4108\u001b[39m     \u001b[38;5;66;03m# joyfulness), but for now this enough.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4109\u001b[39m     \u001b[43msafe_save_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshard_file\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mformat\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4110\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   4111\u001b[39m     save_function(shard, os.path.join(save_directory, shard_file))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\scout\\anaconda3\\envs\\nlp-env\\Lib\\site-packages\\safetensors\\torch.py:286\u001b[39m, in \u001b[36msave_file\u001b[39m\u001b[34m(tensors, filename, metadata)\u001b[39m\n\u001b[32m    255\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msave_file\u001b[39m(\n\u001b[32m    256\u001b[39m     tensors: Dict[\u001b[38;5;28mstr\u001b[39m, torch.Tensor],\n\u001b[32m    257\u001b[39m     filename: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m    258\u001b[39m     metadata: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    259\u001b[39m ):\n\u001b[32m    260\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    261\u001b[39m \u001b[33;03m    Saves a dictionary of tensors into raw bytes in safetensors format.\u001b[39;00m\n\u001b[32m    262\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    284\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m    285\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m     \u001b[43mserialize_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_flatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mSafetensorError\u001b[39m: Error while serializing: IoError(Os { code: 1224, kind: Uncategorized, message: \"The requested operation cannot be performed on a file with a user-mapped section open.\" })"
     ]
    }
   ],
   "source": [
    "model.save_pretrained(\"./modelo_poemas\")\n",
    "tokenizer.save_pretrained(\"./modelo_poemas\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb213c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# Cargar modelo entrenado\n",
    "modelo_entrenado = BertForSequenceClassification.from_pretrained(\"./modelo_poemas\")\n",
    "tokenizer_entrenado = BertTokenizer.from_pretrained(\"./modelo_poemas\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f389d5a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicción: haiku\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "poema = \"\"\"One of the four great masters of Japanese haiku, Matsuo Bashō is known for his simplistic yet thought-provoking haikus. “The Old Pond”, arguably his most famous piece, stays true to his style of couching observations of human nature within natural imagery. One interpretation is that by metaphorically using the ‘pond’ to symbolize the mind, Bashō brings to light the impact of external stimuli (embodied by the frog, a traditional subject of Japanese poetry) on the human mind. \n",
    "\"\"\"\n",
    "\n",
    "# Preparar input\n",
    "inputs = tokenizer_entrenado(poema, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=128)\n",
    "inputs = {k: v.to(modelo_entrenado.device) for k, v in inputs.items()}\n",
    "\n",
    "# Predecir\n",
    "modelo_entrenado.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = modelo_entrenado(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predicted_class_id = logits.argmax().item()\n",
    "\n",
    "print(\"Predicción:\", le.classes_[predicted_class_id])  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89db3c12",
   "metadata": {},
   "source": [
    "<div style=\"background:#FFFFE0;padding:20px;color:#000000;margin-top:10px;\">\n",
    "Poema en Sonnet para prueba: https://www.poetryfoundation.org/poems/45087/sonnet-18-shall-i-compare-thee-to-a-summers-day</div>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
