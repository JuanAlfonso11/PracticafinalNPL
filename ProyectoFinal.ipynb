{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80811e2d",
   "metadata": {},
   "source": [
    "<div style=\"background:#FFFFE0;padding:20px;color:#000000;margin-top:10px;\">\n",
    "Imports necesarios para la ejecución de los módulos instalados con pip:\n",
    "\n",
    "• pandas → import pandas as pd  \n",
    "• numpy → import numpy as np  \n",
    "• matplotlib → import matplotlib.pyplot as plt  \n",
    "• seaborn → import seaborn as sns  \n",
    "• scikit-learn → from sklearn.model_selection import train_test_split  \n",
    "                    from sklearn.metrics import classification_report, confusion_matrix  \n",
    "• torch → import torch  \n",
    "• transformers → from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding  \n",
    "• datasets → from datasets import Dataset  \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b657d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install transformers datasets scikit-learn pandas matplotlib seaborn torch\n",
    "#!pip install \"transformers[torch]\" --upgrade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee116fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\scout\\anaconda3\\envs\\nlp-env\\lib\\site-packages (4.54.0)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.54.1-py3-none-any.whl.metadata (41 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\scout\\anaconda3\\envs\\nlp-env\\lib\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\scout\\anaconda3\\envs\\nlp-env\\lib\\site-packages (from transformers) (0.34.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\scout\\anaconda3\\envs\\nlp-env\\lib\\site-packages (from transformers) (2.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\scout\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\scout\\anaconda3\\envs\\nlp-env\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\scout\\anaconda3\\envs\\nlp-env\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\scout\\anaconda3\\envs\\nlp-env\\lib\\site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\scout\\anaconda3\\envs\\nlp-env\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\scout\\anaconda3\\envs\\nlp-env\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\scout\\anaconda3\\envs\\nlp-env\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\scout\\anaconda3\\envs\\nlp-env\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\scout\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\scout\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\scout\\anaconda3\\envs\\nlp-env\\lib\\site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\scout\\anaconda3\\envs\\nlp-env\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\scout\\anaconda3\\envs\\nlp-env\\lib\\site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\scout\\anaconda3\\envs\\nlp-env\\lib\\site-packages (from requests->transformers) (2025.4.26)\n",
      "Downloading transformers-4.54.1-py3-none-any.whl (11.2 MB)\n",
      "   ---------------------------------------- 0.0/11.2 MB ? eta -:--:--\n",
      "   ------------------- -------------------- 5.5/11.2 MB 37.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.2/11.2 MB 41.0 MB/s eta 0:00:00\n",
      "Installing collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.54.0\n",
      "    Uninstalling transformers-4.54.0:\n",
      "      Successfully uninstalled transformers-4.54.0\n",
      "Successfully installed transformers-4.54.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spacy-transformers 1.3.9 requires transformers<4.50.0,>=3.4.0, but you have transformers 4.54.1 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "#!pip install \"transformers[torch]\" --upgrade\n",
    "#!pip install --upgrade transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a3e1816",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade transformers accelerate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692cb8fa",
   "metadata": {},
   "source": [
    "<div style=\"background:#FFFFE0;padding:20px;color:#000000;margin-top:10px;\">\n",
    "Este bloque de código verifica si PyTorch puede usar la GPU (usualmente con CUDA) y cuál GPU está disponible. Es útil para asegurarse de que el entrenamiento del modelo se pueda hacer con aceleración por hardware, lo que reduce significativamente el tiempo.</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b851e5bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA disponible: True\n",
      "GPU: NVIDIA GeForce RTX 3050 Ti Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"CUDA disponible:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdec902a",
   "metadata": {},
   "source": [
    "# Clasificacion automatica de poemas segun su forma poetica(usando la carpeta forms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41fd639c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from transformers import DataCollatorWithPadding\n",
    "from datasets import Dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "373ddf2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text label\n",
      "0  2 ABC of H.k. and China revised vision.\\nBarre...   abc\n",
      "1  Apparently life without love, is no life at al...   abc\n",
      "2  A abc angles on angels flaws (poem)\\nMix with ...   abc\n",
      "3  A abc Brazil dance (poem)\\nJack of crack in po...   abc\n",
      "4  ABC... I can't go on\\n123... what's the next o...   abc\n",
      "label\n",
      "acrostic                       100\n",
      "allegory                       100\n",
      "free-verse                     100\n",
      "cinquain                       100\n",
      "cavatina                       100\n",
      "ballad                         100\n",
      "ballade                        100\n",
      "tetractys                      100\n",
      "triolet                        100\n",
      "villanelle                     100\n",
      "stanza                         100\n",
      "syllabic-verse                 100\n",
      "epigram                        100\n",
      "dirge                          100\n",
      "clerihew                       100\n",
      "epitaph                        100\n",
      "elegy                          100\n",
      "epistle                        100\n",
      "verse                          100\n",
      "monoku                         100\n",
      "lament                         100\n",
      "italian-sonnet                 100\n",
      "hymn                           100\n",
      "tanka                          100\n",
      "couplet                         99\n",
      "sestina                         99\n",
      "doggerel                        99\n",
      "abc                             99\n",
      "epic                            99\n",
      "riddle                          99\n",
      "lyric                           99\n",
      "carol                           99\n",
      "haiku                           99\n",
      "ghazal                          99\n",
      "pastoral                        98\n",
      "limerick                        98\n",
      "quatrain                        98\n",
      "ode                             97\n",
      "imagery                         96\n",
      "pantoum                         96\n",
      "rondeau                         90\n",
      "dactyl                          83\n",
      "sonnet                          79\n",
      "aubade                          77\n",
      "prose-poem                      77\n",
      "bio                             76\n",
      "narrative                       70\n",
      "madrigal                        68\n",
      "tercet                          66\n",
      "octave                          64\n",
      "rictameter                      63\n",
      "found-poem                      61\n",
      "slam                            59\n",
      "tyburn                          59\n",
      "ars-poetica                     57\n",
      "heroic-couplet                  55\n",
      "kyrielle                        54\n",
      "chain-verse                     52\n",
      "eclogue                         51\n",
      "sijo                            49\n",
      "conceit                         49\n",
      "cacophony                       48\n",
      "cascade                         44\n",
      "quatern                         36\n",
      "anaphora                        30\n",
      "lay                             29\n",
      "anacreontic                     29\n",
      "blank-verse                     29\n",
      "sestet                          28\n",
      "anagram                         26\n",
      "nonet                           26\n",
      "shi                             25\n",
      "light-verse                     25\n",
      "renga                           25\n",
      "bop                             24\n",
      "shadorma                        24\n",
      "ekphrastic                      22\n",
      "terza-rima                      22\n",
      "abecedarian                     20\n",
      "iambic-pentameter               20\n",
      "burlesque                       19\n",
      "canzone                         19\n",
      "choka                           18\n",
      "rhyme-royal-or-rime-royale      18\n",
      "catena-rondo                    18\n",
      "blues-poem                      17\n",
      "bucolic                         17\n",
      "dizain                          17\n",
      "rispetto                        15\n",
      "dramatic-monologue              15\n",
      "ottava-rima                     15\n",
      "balassi-stanza                  14\n",
      "sapphic                         13\n",
      "palinode                        12\n",
      "cento                           12\n",
      "chance-operations               12\n",
      "alexandrine                     11\n",
      "double-dactyl                   11\n",
      "epithalamion                    10\n",
      "senryu                          10\n",
      "panegyric                       10\n",
      "verse-paragraph                  9\n",
      "kennings                         9\n",
      "somonka                          9\n",
      "palindrome-or-mirror-poetry      9\n",
      "refrain                          9\n",
      "landays                          8\n",
      "horatian-ode                     7\n",
      "arabian-sonnet                   6\n",
      "bref-double                      6\n",
      "curtal-sonnet                    6\n",
      "glosa                            5\n",
      "burns-stanza                     5\n",
      "spenserian-stanza                5\n",
      "canzonetta                       4\n",
      "decastich                        4\n",
      "echo-verse                       4\n",
      "beymorlin-sonnet                 4\n",
      "brisbane-sonnet                  4\n",
      "oulipo                           4\n",
      "qasida                           4\n",
      "rondel-or-roundel                4\n",
      "didactic-poetry                  3\n",
      "collins-sestet                   3\n",
      "carpe-diems                      3\n",
      "blues-sonnet                     2\n",
      "busta-sonetto                    2\n",
      "fourteener                       2\n",
      "epistrophe                       2\n",
      "divino-sonetto                   2\n",
      "occasional-poem                  2\n",
      "pindaric-ode                     2\n",
      "mock-epic                        1\n",
      "irregular-ode                    1\n",
      "triversen                        1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "ruta_base = \"archive/forms\"\n",
    "\n",
    "# Inicializamos listas vacías\n",
    "textos = []\n",
    "etiquetas = []\n",
    "\n",
    "# Recorrer cada carpeta (que es una clase)\n",
    "for nombre_carpeta in os.listdir(ruta_base):\n",
    "    ruta_carpeta = os.path.join(ruta_base, nombre_carpeta)\n",
    "    if os.path.isdir(ruta_carpeta):\n",
    "        for archivo in os.listdir(ruta_carpeta):\n",
    "            ruta_archivo = os.path.join(ruta_carpeta, archivo)\n",
    "            try:\n",
    "                with open(ruta_archivo, 'r', encoding='utf-8') as f:\n",
    "                    contenido = f.read().strip()\n",
    "                    textos.append(contenido)\n",
    "                    etiquetas.append(nombre_carpeta)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "# Crear el DataFrame\n",
    "df = pd.DataFrame({'text': textos, 'label': etiquetas})\n",
    "\n",
    "# Ver los primeros datos\n",
    "print(df.head())\n",
    "print(df['label'].value_counts().to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7f7b282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "otros     6140\n",
      "haiku       99\n",
      "sonnet      79\n",
      "Name: count, dtype: int64\n",
      "label_id\n",
      "1    6140\n",
      "0      99\n",
      "2      79\n",
      "Name: count, dtype: int64\n",
      "['haiku' 'otros' 'sonnet']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Etiquetas que sí quieres mantener con su nombre\n",
    "clases_deseadas = ['haiku', 'sonnet']\n",
    "\n",
    "# Reasignamos todo lo que no es haiku ni sonnet como \"otros\"\n",
    "df['label'] = df['label'].apply(lambda x: x if x in clases_deseadas else 'otros')\n",
    "\n",
    "# Ahora sí codificamos las tres clases\n",
    "le = LabelEncoder()\n",
    "df['label_id'] = le.fit_transform(df['label'])\n",
    "\n",
    "# Imprimir para verificar\n",
    "print(df['label'].value_counts())\n",
    "print(df['label_id'].value_counts())\n",
    "print(le.classes_)  # Te dirá cuál clase es cuál número\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d8581b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 6318/6318 [00:22<00:00, 276.68 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Cargar el tokenizer de BERT\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=128)\n",
    "\n",
    "dataset = Dataset.from_pandas(df[['text', 'label_id']])\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.rename_column(\"label_id\", \"labels\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "60311ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenamiento: 5054 ejemplos\n",
      "Evaluación: 1264 ejemplos\n"
     ]
    }
   ],
   "source": [
    "# Dividir en entrenamiento y prueba (80% - 20%)\n",
    "split_dataset = tokenized_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# Asignar a variables por claridad\n",
    "train_dataset = split_dataset['train']\n",
    "eval_dataset = split_dataset['test']\n",
    "\n",
    "# Confirmar tamaños\n",
    "print(f\"Entrenamiento: {len(train_dataset)} ejemplos\")\n",
    "print(f\"Evaluación: {len(eval_dataset)} ejemplos\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bbe3ca2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "# Indicar la cantidad de clases (por ejemplo: 3 si tienes haiku, sonnet y otros)\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae0c0b8",
   "metadata": {},
   "source": [
    "con 3 epecas hay confuncion en varios poemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9f92b199",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scout\\AppData\\Local\\Temp\\ipykernel_18764\\2977003183.py:17: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3160' max='3160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3160/3160 11:52, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.061800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.022300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.014400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.008800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.004700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.003100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\scout\\anaconda3\\envs\\nlp-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "c:\\Users\\scout\\anaconda3\\envs\\nlp-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "c:\\Users\\scout\\anaconda3\\envs\\nlp-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "c:\\Users\\scout\\anaconda3\\envs\\nlp-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "c:\\Users\\scout\\anaconda3\\envs\\nlp-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "c:\\Users\\scout\\anaconda3\\envs\\nlp-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3160, training_loss=0.018526437275017363, metrics={'train_runtime': 712.8334, 'train_samples_per_second': 35.45, 'train_steps_per_second': 4.433, 'total_flos': 1662219016496640.0, 'train_loss': 0.018526437275017363, 'epoch': 5.0})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "# Preparar colador de datos\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Argumentos básicos de entrenamiento compatibles\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./resultados\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    logging_dir=\"./logs\"\n",
    ")\n",
    "\n",
    "# Definir Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Entrenar\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8b54ebcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./modelo_poemasv2\\\\tokenizer_config.json',\n",
       " './modelo_poemasv2\\\\special_tokens_map.json',\n",
       " './modelo_poemasv2\\\\vocab.txt',\n",
       " './modelo_poemasv2\\\\added_tokens.json')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./modelo_poemasv2\")\n",
    "tokenizer.save_pretrained(\"./modelo_poemasv2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dfb213c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# Cargar modelo entrenado\n",
    "modelo_entrenado = BertForSequenceClassification.from_pretrained(\"./modelo_poemasv2\")\n",
    "tokenizer_entrenado = BertTokenizer.from_pretrained(\"./modelo_poemasv2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f389d5a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicción: otros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\scout\\anaconda3\\envs\\nlp-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "poema = \"\"\"One of the four great masters of Japanese haiku, Matsuo Bashō is known for his simplistic yet thought-provoking haikus. “The Old Pond”, arguably his most famous piece, stays true to his style of couching observations of human nature within natural imagery. One interpretation is that by metaphorically using the ‘pond’ to symbolize the mind, Bashō brings to light the impact of external stimuli (embodied by the frog, a traditional subject of Japanese poetry) on the human mind. \n",
    "\"\"\"\n",
    "\n",
    "# Preparar input\n",
    "inputs = tokenizer_entrenado(poema, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=128)\n",
    "inputs = {k: v.to(modelo_entrenado.device) for k, v in inputs.items()}\n",
    "\n",
    "# Predecir\n",
    "modelo_entrenado.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = modelo_entrenado(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predicted_class_id = logits.argmax().item()\n",
    "\n",
    "print(\"Predicción:\", le.classes_[predicted_class_id])  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c6014800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "haiku10_masaoka_shiki_cold.txt: sonnet (confianza: 1.00)\n",
      "haiku11_modern_sea.txt: sonnet (confianza: 1.00)\n",
      "haiku12_modern_lanterns.txt: sonnet (confianza: 1.00)\n",
      "haiku13_modern_train.txt: sonnet (confianza: 1.00)\n",
      "haiku14_modern_rain.txt: sonnet (confianza: 1.00)\n",
      "haiku15_modern_moon.txt: haiku (confianza: 1.00)\n",
      "haiku16_modern_street.txt: haiku (confianza: 1.00)\n",
      "haiku17_modern_coffee.txt: sonnet (confianza: 1.00)\n",
      "haiku18_modern_tree.txt: sonnet (confianza: 1.00)\n",
      "haiku19_modern_beach.txt: sonnet (confianza: 1.00)\n",
      "haiku1_matsuo_basho_frog.txt: haiku (confianza: 1.00)\n",
      "haiku20_modern_fireflies.txt: sonnet (confianza: 1.00)\n",
      "haiku2_matsuo_basho_autumn.txt: haiku (confianza: 1.00)\n",
      "haiku3_matsuo_basho_summer.txt: sonnet (confianza: 1.00)\n",
      "haiku4_yosa_buson_butterfly.txt: sonnet (confianza: 1.00)\n",
      "haiku5_yosa_buson_moon.txt: sonnet (confianza: 1.00)\n",
      "haiku6_kobayashi_issa_snail.txt: sonnet (confianza: 1.00)\n",
      "haiku7_kobayashi_issa_dewdrop.txt: sonnet (confianza: 1.00)\n",
      "haiku8_kobayashi_issa_child.txt: sonnet (confianza: 1.00)\n",
      "haiku9_masaoka_shiki_cherry.txt: haiku (confianza: 1.00)\n",
      "poem01_free_verse_morning.txt: sonnet (confianza: 1.00)\n",
      "poem02_limerick_cat.txt: sonnet (confianza: 1.00)\n",
      "poem03_acrostic_peace.txt: sonnet (confianza: 1.00)\n",
      "poem04_narrative_village.txt: sonnet (confianza: 1.00)\n",
      "poem05_tanka_autumn.txt: sonnet (confianza: 1.00)\n",
      "poem06_concrete_tree.txt: sonnet (confianza: 1.00)\n",
      "poem07_elegy_evening.txt: haiku (confianza: 1.00)\n",
      "poem08_couplet_night.txt: haiku (confianza: 1.00)\n",
      "poem09_epigram_truth.txt: sonnet (confianza: 1.00)\n",
      "poem10_ballad_forest.txt: sonnet (confianza: 1.00)\n",
      "poem11_shape_heart.txt: haiku (confianza: 0.99)\n",
      "poem12_villanelle_wind.txt: sonnet (confianza: 1.00)\n",
      "poem13_blank_verse_clouds.txt: sonnet (confianza: 1.00)\n",
      "poem14_epic_stars.txt: sonnet (confianza: 1.00)\n",
      "poem15_didactic_mirror.txt: sonnet (confianza: 1.00)\n",
      "poem16_ode_to_rain.txt: sonnet (confianza: 1.00)\n",
      "poem17_pastoral_field.txt: sonnet (confianza: 1.00)\n",
      "poem18_quatrain_stars.txt: sonnet (confianza: 1.00)\n",
      "poem19_chant_dawn.txt: sonnet (confianza: 1.00)\n",
      "poem20_list_nature.txt: sonnet (confianza: 0.77)\n",
      "sonnet01_shakespeare_18.txt: otros (confianza: 1.00)\n",
      "sonnet02_shakespeare_116.txt: otros (confianza: 1.00)\n",
      "sonnet03_shakespeare_130.txt: sonnet (confianza: 1.00)\n",
      "sonnet04_milton_on_his_blindness.txt: sonnet (confianza: 1.00)\n",
      "sonnet05_modern_classical.txt: sonnet (confianza: 1.00)\n",
      "sonnet06_modern_classical.txt: sonnet (confianza: 1.00)\n",
      "sonnet07_modern_classical.txt: sonnet (confianza: 1.00)\n",
      "sonnet08_modern_classical.txt: sonnet (confianza: 1.00)\n",
      "sonnet09_modern_classical.txt: sonnet (confianza: 1.00)\n",
      "sonnet10_modern_classical.txt: sonnet (confianza: 1.00)\n",
      "sonnet11_modern_classical.txt: sonnet (confianza: 1.00)\n",
      "sonnet12_modern_classical.txt: sonnet (confianza: 1.00)\n",
      "sonnet13_modern_classical.txt: sonnet (confianza: 1.00)\n",
      "sonnet14_modern_classical.txt: sonnet (confianza: 1.00)\n",
      "sonnet15_modern_classical.txt: sonnet (confianza: 1.00)\n",
      "sonnet16_modern_classical.txt: sonnet (confianza: 1.00)\n",
      "sonnet17_modern_classical.txt: sonnet (confianza: 1.00)\n",
      "sonnet18_modern_classical.txt: sonnet (confianza: 1.00)\n",
      "sonnet19_modern_classical.txt: sonnet (confianza: 1.00)\n",
      "sonnet20_modern_classical.txt: sonnet (confianza: 1.00)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from pathlib import Path\n",
    "\n",
    "# Cargar modelo y tokenizer\n",
    "model_path = \"./modelo_poemasv2\"\n",
    "model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "model.eval()\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Clases en el mismo orden del entrenamiento\n",
    "labels = [\"haiku\", \"sonnet\", \"otros\"]\n",
    "\n",
    "# Dispositivo\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Carpeta con los archivos .txt\n",
    "carpeta_poemas = \"./Poemasvar\"\n",
    "\n",
    "# Umbral de confianza para decir \"desconocido\"\n",
    "umbral_confianza = 0.65  # puedes ajustar entre 0.5 y 0.7 según tu preferencia\n",
    "\n",
    "# Recorrer todos los archivos .txt\n",
    "for archivo in Path(carpeta_poemas).glob(\"*.txt\"):\n",
    "    with open(archivo, \"r\", encoding=\"utf-8\") as f:\n",
    "        texto = f.read().strip()\n",
    "\n",
    "    # Tokenizar\n",
    "    inputs = tokenizer(texto, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=128)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    # Predicción\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        confidence, pred_id = torch.max(probs, dim=1)\n",
    "\n",
    "    # Evaluar confianza\n",
    "    if confidence.item() < umbral_confianza:\n",
    "        clase_predicha = \"desconocido\"\n",
    "    else:\n",
    "        clase_predicha = labels[pred_id.item()]\n",
    "\n",
    "    print(f\"{archivo.name}: {clase_predicha} (confianza: {confidence.item():.2f})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
